# üìö Philosopher RAG

> **Grounded Question Answering over Schopenhauer's Texts using Endee Vector DB**

A production-quality RAG system that answers philosophical questions about Arthur Schopenhauer's *The World as Will and Representation* by retrieving relevant passages from a vector database and generating grounded responses via an LLM.

Built with **10 advanced RAG techniques** and powered by **Endee** ‚Äî a high-performance open-source vector database with native hybrid search support.

---

## ‚ú® Features

| # | Feature | Description |
|:-:|---------|-------------|
| 1 | **Hybrid Search** | Dense (e5-large-v2, 1024-dim) + Sparse (TF-IDF) retrieval via Endee's native hybrid index |
| 2 | **Re-ranking** | LLM-based passage scoring for improved precision |
| 3 | **Evaluation Pipeline** | 10 curated questions with automated concept-hit-rate metrics |
| 4 | **Metadata Filtering** | Filter search results by volume using Endee's filter API |
| 5 | **Hierarchical Chunking** | Parent (3K chars) ‚Üí Child (800 chars) for context-rich retrieval |
| 6 | **HyDE** | Hypothetical Document Embeddings for better recall on abstract queries |
| 7 | **Query Expansion** | Multi-query reformulation and result merging |
| 8 | **Agentic RAG** | Multi-step retrieval with automated quality assessment and query refinement |
| 9 | **Streaming Output** | Token-by-token LLM response for responsive UX |
| 10 | **Conversation Mode** | Multi-turn interactive chat with memory |

---

## üèóÔ∏è Architecture

```
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ    Documents/ (3 volumes)     ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ  Hierarchical    ‚îÇ
                              ‚îÇ  Chunking        ‚îÇ
                              ‚îÇ  Parent ‚Üí Child  ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                  ‚îÇ                   ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ  Dense Embed   ‚îÇ  ‚îÇ TF-IDF Sparse ‚îÇ  ‚îÇ  Metadata    ‚îÇ
           ‚îÇ (e5-large-v2)  ‚îÇ  ‚îÇ   Encoding    ‚îÇ  ‚îÇ  Filters     ‚îÇ
           ‚îÇ   1024-dim     ‚îÇ  ‚îÇ   ~24K-dim    ‚îÇ  ‚îÇ  (volume)    ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ   Endee DB       ‚îÇ
                      ‚îÇ  Hybrid HNSW     ‚îÇ
                      ‚îÇ  (cosine sim)    ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
     User Question ‚îÄ‚îÄ‚ñ∫ [HyDE] ‚îÄ‚îÄ‚ñ∫ [Expand] ‚îÄ‚îÄ‚ñ∫ Hybrid Search ‚îÄ‚îÄ‚ñ∫ [Re-rank]
                                                                     ‚îÇ
                                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                              ‚îÇ  LLM (Llama3‚îÇ
                                                              ‚îÇ  via Ollama)‚îÇ
                                                              ‚îÇ  + Sources  ‚îÇ
                                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why Endee?

This project demonstrates deep usage of Endee's capabilities:
- **Hybrid Index** ‚Äî Dense + sparse vectors in a single index for combined semantic and lexical search
- **Metadata Filtering** ‚Äî Targeted retrieval by volume using Endee's filter API
- **Batch Upsert** ‚Äî Efficient batch vector operations via the Python SDK
- **HNSW Search** ‚Äî Fast approximate nearest neighbor search with cosine similarity

---

## üöÄ Quick Start

### Prerequisites

| Requirement | Purpose |
|-------------|---------|
| Python 3.10+ | Runtime |
| [Endee](https://github.com/endee-ai/endee) | Vector database |
| [Ollama](https://ollama.ai) | LLM inference (Llama3) |

### 1. Install Dependencies

```bash
# Create and activate a virtual environment (recommended)
python -m venv venv
source venv/bin/activate

# Install packages
pip install -r requirements.txt
```

### 2. Pull the LLM Model

```bash
ollama pull llama3
```

### 3. Start the Endee Server

```bash
cd endee_fork/endee
./run.sh
```

### 4. Configure (Optional)

```bash
cp .env.example .env
# Edit .env to customize:
#   ENDEE_URL        ‚Äî Endee server URL (default: http://localhost:8080/api/v1)
#   OLLAMA_MODEL     ‚Äî LLM model name (default: llama3)
#   EMBEDDING_MODEL  ‚Äî HuggingFace model ID or local path
#                      (default: intfloat/e5-large-v2, auto-downloaded on first run)
```

### 5. Index the Documents

```bash
python run.py ingest
```

This runs the full ingestion pipeline:
- Loads 3 volumes of Schopenhauer's text
- Creates hierarchical chunks (parent ‚Üí child)
- Generates dense embeddings (e5-large-v2, 1024-dim)
- Builds TF-IDF sparse vectors
- Stores everything in Endee's hybrid index with metadata filters

### 6. Ask Questions

```bash
python run.py query "Why does Schopenhauer believe suffering is fundamental to life?"
```

---

## üìñ Usage Guide

### Basic Query

```bash
python run.py query "What is the role of art in Schopenhauer's philosophy?"
```

### Advanced Query Flags

| Flag | Feature | What it Does |
|------|---------|--------------|
| `--hyde` | HyDE | Generates a hypothetical answer, then embeds *that* for better recall |
| `--expand` | Query Expansion | Creates 3 alternative phrasings, searches all, merges results |
| `--rerank` | Re-ranking | LLM scores each passage 1-10 and re-sorts by relevance |
| `--stream` | Streaming | Prints the LLM response token-by-token |
| `--filter vol1` | Metadata Filter | Restricts search to a specific volume (vol1, vol2, vol3) |
| `--all` | Everything | Enables HyDE + expand + rerank + stream together |
| `--top_k N` | Result Count | Number of passages to retrieve (default: 5) |

```bash
# Use all advanced features at once
python run.py query "What is the will?" --all

# Filter to Volume 1 only
python run.py query "What does Schopenhauer say about music?" --filter vol1

# Just re-rank and stream
python run.py query "What is the denial of the will to live?" --rerank --stream
```

### Conversation Mode

Interactive multi-turn chat with context memory. The assistant remembers your previous questions.

```bash
python run.py chat
```

```
  You: What does Schopenhauer think about art?
  Assistant: According to Schopenhauer, art serves as a temporary escape from...

  You: How does music differ from other art forms?
  Assistant: Building on what we discussed, Schopenhauer gives music a special status...

  You: quit
```

### Agentic Mode

Multi-step reasoning with automatic quality assessment. The agent evaluates whether retrieved passages are sufficient, and if not, reformulates the query and retrieves again (up to 3 iterations).

```bash
python run.py agent "What is the relationship between will, suffering, and salvation?"
```

### Evaluation

Runs 10 curated philosophical questions and measures concept-hit-rate in retrieved passages.

```bash
python run.py evaluate
```

```
  EVALUATION REPORT
  Questions:              10
  Passing (‚â•60% hits):    8/10
  Overall concept hit:    42/50 (84%)
  Avg retrieval time:     0.15s
```

---

## üß† How It Works

### Ingestion Pipeline

```
Documents/*.txt ‚Üí Hierarchical Chunking ‚Üí Dense Embedding ‚Üí TF-IDF Sparse ‚Üí Endee Hybrid Index
```

1. **Load** ‚Äî Reads plain-text files from `Documents/`
2. **Hierarchical Chunk** ‚Äî Splits into large parent chunks (~3000 chars) then small child chunks (~800 chars). Children are stored as vectors; parents are preserved in metadata for LLM context
3. **Dense Embed** ‚Äî `intfloat/e5-large-v2` with `"passage: "` prefix (1024-dim)
4. **Sparse Encode** ‚Äî TF-IDF vectorizer fit on the full corpus (vocabulary saved to `data/tfidf_vectorizer.pkl`)
5. **Store** ‚Äî Upserted into Endee with dense vector, sparse vector, metadata, and volume filter

### Query Pipeline

```
Question ‚Üí [HyDE] ‚Üí [Expand] ‚Üí Embed ‚Üí Hybrid Search (dense+sparse) ‚Üí [Filter] ‚Üí [Re-rank] ‚Üí LLM ‚Üí Answer
```

1. **HyDE** *(optional)* ‚Äî LLM generates a hypothetical passage; that passage is embedded instead of the raw question
2. **Query Expansion** *(optional)* ‚Äî LLM generates 3 alternative phrasings; all are searched and results merged
3. **Embed** ‚Äî `"query: "` prefix for asymmetric E5 retrieval
4. **Hybrid Search** ‚Äî Endee combines dense cosine similarity with sparse TF-IDF matching
5. **Filter** *(optional)* ‚Äî Endee filters results by volume metadata
6. **Re-rank** *(optional)* ‚Äî LLM scores each passage's relevance 1-10
7. **Generate** ‚Äî Grounded answer using retrieved parent context, with source citations

### Agentic Pipeline

```
Question ‚Üí Retrieve ‚Üí Re-rank ‚Üí Assess sufficiency ‚Üí [Refine query ‚Üí Retrieve again] ‚Üí Answer
```

The agent loops up to 3 times, accumulating unique passages across iterations.

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ Documents/                # Schopenhauer text files (3 volumes)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py             # Ingestion: hierarchical chunk ‚Üí embed ‚Üí sparse ‚Üí store
‚îÇ   ‚îú‚îÄ‚îÄ query.py              # Query: HyDE, expand, hybrid search, rerank, stream, chat
‚îÇ   ‚îú‚îÄ‚îÄ agent.py              # Agentic multi-step RAG
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py           # Evaluation pipeline with concept-hit metrics
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Embedding abstraction (e5-large-v2)
‚îÇ   ‚îú‚îÄ‚îÄ endee_client.py       # Endee SDK wrapper (hybrid search + filtering)
‚îÇ   ‚îú‚îÄ‚îÄ sparse.py             # TF-IDF sparse encoder for hybrid search
‚îÇ   ‚îú‚îÄ‚îÄ reranker.py           # LLM-based re-ranking via Ollama
‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # Document loading + hierarchical chunking
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ tfidf_vectorizer.pkl  # Saved TF-IDF vectorizer (generated during ingest)
‚îú‚îÄ‚îÄ eval_questions.json       # 10 curated evaluation questions
‚îú‚îÄ‚îÄ run.py                    # CLI entry point
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ .env.example              # Environment variable template
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Configuration

All settings are configurable via environment variables or `.env` file:

| Variable | Default | Description |
|----------|---------|-------------|
| `ENDEE_URL` | `http://localhost:8080/api/v1` | Endee server URL |
| `OLLAMA_MODEL` | `llama3` | Ollama model for generation |
| `EMBEDDING_MODEL` | `intfloat/e5-large-v2` | HuggingFace model ID or local path to embedding model |

### Embedding Model

The default embedding model (`intfloat/e5-large-v2`) is automatically downloaded from HuggingFace on first run (~1.3 GB). If you prefer a different model or have a local copy:

```bash
# Use a different HuggingFace model
export EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Use a local model directory
export EMBEDDING_MODEL=/path/to/your/local/model
```

> **Note:** If using a non-E5 model, the `"query: "` / `"passage: "` prefixes may not be optimal. E5 models are specifically trained for this asymmetric retrieval pattern.

---

## üìä Evaluation Details

The evaluation suite (`eval_questions.json`) contains 10 questions spanning key Schopenhauer topics:

- Will and representation
- Suffering and pessimism
- Art, music, and aesthetics
- Ethics and compassion
- Free will and determinism
- Platonic Ideas
- Time, space, and principium individuationis

Each question includes expected concepts. The pipeline checks whether retrieved passages contain these concepts, measuring **concept hit rate** as a proxy for retrieval quality.

---

## üõ†Ô∏è Tech Stack

| Component | Technology |
|-----------|------------|
| Vector Database | [Endee](https://github.com/endee-ai/endee) (hybrid HNSW) |
| Dense Embeddings | intfloat/e5-large-v2 via sentence-transformers |
| Sparse Encoding | TF-IDF via scikit-learn |
| LLM | Llama3 via Ollama |
| Language | Python 3.10+ |
